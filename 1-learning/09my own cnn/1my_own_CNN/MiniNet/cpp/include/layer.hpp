
#ifndef MINI_NET_LAYER_HPP_
#define MINI_NET_LAYER_HPP_

#include "blob.hpp"
#include <memory>
using std::vector;
using std::shared_ptr;

namespace mini_net {

/*! layer parameters */
struct Param {
    Param() : conv_stride(0), conv_pad(0) {}
    /*! \brief conv param */
    int conv_stride;
    int conv_pad;
    int conv_width;
    int conv_height;
    int conv_kernels;
    inline void setConvParam(int stride, int pad, int width, int height, int kernels) {
        conv_stride = stride;
        conv_pad = pad;
        conv_width = width;
        conv_height = height;
        conv_kernels = kernels;
    }
    /*! \brief pool param */
    int pool_stride;
    int pool_width;
    int pool_height;
    inline void setPoolParam(int stride, int width, int height) {
        pool_stride = stride;
        pool_width = width;
        pool_height = height;
    }
    /*! \brief dropout param */
    /*! if the most right bit is 1 use train mode, else use test mode;
     *  if the second bit from right is 1, use random seed; else use selected seed. */
    int drop_mode;
    double drop_p;
    int drop_seed;
    shared_ptr<Blob> drop_mask;
    inline void setDropoutpParam(int mode, double pp, int s) {
        drop_mode = mode;
        drop_p = pp;
        drop_seed = s;
        drop_mask.reset();
    }
    /*! fc parameters */
    int fc_kernels;
};

/*!
 * \brief Affine Layer
 */
class AffineLayer {
public:
    AffineLayer() {}
    ~AffineLayer() {}

    /*!
    * \brief forward
    * Blob bottom[0]:                                                                 in[1]:weight
    *     _______          _______         __  _______________________         __      __  _______________________          __
    *  C /______/|   N    /______/|        |  |_______________________| __      |      |  |_______________________| __       |
    *   |------||| ······|------|||   ===> |          ...                |      |   *  |            ...              |       | . T() + b
    * H |------|||       |------|||   ===> |   _______________________    > N   |      |   _______________________    > F    |
    *   |------|/        |------|/         |_ |_______________________| _|     _|      |_ |_______________________| _|      _|
    *      W                                                                                         
    *   \___________  __________/             \___________  __________/                  \___________  __________/          
    *               \/                                    \/                                         \/
    *           [N,C,H,W]                               C*H*W                                       C*H*N
    *
    *             X:        [N, C, Hx, Wx]
    *             weight:   [F, C, Hw, Ww]
    *             bias:     [F, 1, 1, 1]
    *             out:      [N, F, 1, 1]
    * \param[in]  const vector<Blob*>& in       in[0]:X, in[1]:weights, in[2]:bias
    * \param[out] Blob& out                     Y
    */
    static void forward(const vector<shared_ptr<Blob>>& in,
                        shared_ptr<Blob>& out);

    /*!
    * \brief backward
    *             in:       [N, C, Hx, Wx]
    *             weight:   [F, C, Hw, Ww]
    *             bias:     [F, 1, 1, 1]
    *             dout:     [N, F, 1, 1]
    * \param[in]  const Blob* dout              dout
    * \param[in]  const vector<Blob*>& cache    cache[0]:X, cache[1]:weights, cache[2]:bias
    * \param[out] vector<Blob*>& grads          grads[0]:dX, grads[1]:dW, grads[2]:db
    */
    static void backward(shared_ptr<Blob>& dout,
                         const vector<shared_ptr<Blob>>& cache,
                         vector<shared_ptr<Blob>>& grads);
};

/*!
* \brief Convolutional Layer
*/
class ConvLayer {
public:
    ConvLayer() {}
    ~ConvLayer() {}

    /*!
    * \brief forward
    *  Blob bottom[0]:
    *     _______          _______                                               _______          _______
    *  C /______/|   N    /______/|                                           C /______/|   N*F  /______/| 
    *   |------||| ······|------|||                                            |------||| ······|------|||
    * H |------|||       |------|||    *   F个kernel size为(n,n)的卷积核  =  Hw |------|||       |------||| Hw
    *   |------|/        |------|/                                             |------|/        |------|/
    *      W                                                                      Ww               Ww
    *   \___________  __________/
    *               \/
    *            [N,C,H,W]  
    *
    *             X:        [N, C, Hx, Wx]
    *             weight:   [F, C, Hw, Ww]
    *             bias:     [F, 1, 1, 1]
    *             out:      [N, F, (Hx+pad*2-Hw)/stride+1, (Wx+pad*2-Ww)/stride+1]
    * \param[in]  const vector<Blob*>& in       in[0]:X, in[1]:weights, in[2]:bias
    * \param[in]  const ConvParam* param        conv params
    * \param[out] Blob& out                     Y
    */
    static void forward(const vector<shared_ptr<Blob>>& in,
                        shared_ptr<Blob>& out,
                        Param& param);

    /*!
    * \brief backward
    *             in:       [N, C, Hx, Wx]
    *             weight:   [F, C, Hw, Ww]
    *             bias:     [F, 1, 1, 1]
    *             dout:     [N, F, (Hx+pad*2-Hw)/stride+1, (Wx+pad*2-Ww)/stride+1]
    * \param[in]  const Blob* dout              dout
    * \param[in]  const vector<Blob*>& cache    cache[0]:X, cache[1]:weights, cache[2]:bias
    * \param[out] vector<Blob*>& grads          grads[0]:dX, grads[1]:dW, grads[2]:db
    */
    static void backward(shared_ptr<Blob>& dout,
                         const vector<shared_ptr<Blob>>& cache,
                         vector<shared_ptr<Blob>>& grads,
                         Param& param);
};

/*!
* \brief Max Pooling Layer
*/
class PoolLayer {
public:
    PoolLayer() {}
    ~PoolLayer() {}

    /*!
    * \brief forward
    *             X:        [N, C, Hx, Wx]
    *             out:      [N, C, Hx/2, Wx/2]
    * \param[in]  const vector<Blob*>& in       in[0]:X
    * \param[in]  const Param* param        conv params
    * \param[out] Blob& out                     Y
    */
    static void forward(const vector<shared_ptr<Blob>>& in,
                        shared_ptr<Blob>& out,
                        Param& param);

    /*!
    * \brief backward
    *             in:       [N, C, Hx, Wx]
    *             dout:     [N, F, Hx/2, Wx/2]
    * \param[in]  const Blob* dout              dout
    * \param[in]  const vector<Blob*>& cache    cache[0]:X
    * \param[out] vector<Blob*>& grads          grads[0]:dX
    */
    static void backward(shared_ptr<Blob>& dout,
                         const vector<shared_ptr<Blob>>& cache,
                         vector<shared_ptr<Blob>>& grads,
                         Param& param);
};

/*!
* \brief ReLU Layer
*/
class ReluLayer {
public:
    ReluLayer() {}
    ~ReluLayer() {}

    /*!
    * \brief forward, out = max(0, X)
    *             X:        [N, C, Hx, Wx]
    *             out:      [N, C, Hx, Wx]
    * \param[in]  const vector<Blob*>& in       in[0]:X
    * \param[out] Blob& out                     Y
    */
    static void forward(const vector<shared_ptr<Blob>>& in,
                        shared_ptr<Blob>& out);

    /*!
    * \brief backward, dX = dout .* (X > 0)
    *             in:       [N, C, Hx, Wx]
    *             dout:     [N, F, Hx, Wx]
    * \param[in]  const Blob* dout              dout
    * \param[in]  const vector<Blob*>& cache    cache[0]:X
    * \param[out] vector<Blob*>& grads          grads[0]:dX
    */
    static void backward(shared_ptr<Blob>& dout,
                         const vector<shared_ptr<Blob>>& cache,
                         vector<shared_ptr<Blob>>& grads);
};

/*!
* \brief Dropout Layer
*/
class DropoutLayer {
public:
    DropoutLayer() {}
    ~DropoutLayer() {}

    /*!
    * \brief forward
    *             X:        [N, C, Hx, Wx]
    *             out:      [N, C, Hx, Wx]
    * \param[in]  const vector<Blob*>& in       in[0]:X
    * \param[out] Blob& out                     Y
    */
    static void forward(const vector<shared_ptr<Blob>>& in,
                        shared_ptr<Blob>& out,
                        Param& param);

    /*!
    * \brief backward
    *             in:       [N, C, Hx, Wx]
    *             dout:     [N, F, Hx, Wx]
    * \param[in]  const Blob* dout              dout
    * \param[in]  const vector<Blob*>& cache    cache[0]:X
    * \param[out] vector<Blob*>& grads          grads[0]:dX
    */
    static void backward(shared_ptr<Blob>& dout,
                         const vector<shared_ptr<Blob>>& cache,
                         vector<shared_ptr<Blob>>& grads,
                         Param& param);
};

/*!
* \brief Softmax Loss Layer
*/
class SoftmaxLossLayer {
public:
    SoftmaxLossLayer() {}
    ~SoftmaxLossLayer() {}

    /*!
    * \brief forward
    *             X:        [N, C, 1, 1], usually the output of affine(fc) layer
    *             Y:        [N, C, 1, 1], ground truth, with 1(true) or 0(false)
    * \param[in]  const vector<Blob*>& in       in[0]:X, in[1]:Y
    * \param[out] double& loss                  loss
    * \param[out] Blob** out                    out: dX
    * \param[in]  int mode                      1: only forward, 0:forward and backward
    */
    static void go(const vector<shared_ptr<Blob>>& in,
                   double& loss,
                   shared_ptr<Blob>& dout,
                   int mode = 0);
};

/*!
* \brief SVM Loss Layer
*/
class SVMLossLayer {
public:
    SVMLossLayer() {}
    ~SVMLossLayer() {}

    /*!
    * \brief forward
    *             X:        [N, C, 1, 1], usually the output of affine(fc) layer
    *             Y:        [N, C, 1, 1], ground truth, with 1(true) or 0(false)
    * \param[in]  const vector<Blob*>& in       in[0]:X, in[1]:Y
    * \param[out] double& loss                  loss
    * \param[out] Blob** out                    out: dX
    * \param[in]  int mode                      1: only forward, 0:forward and backward
    */
    static void go(const vector<shared_ptr<Blob>>& in,
                   double& loss,
                   shared_ptr<Blob>& dout, 
                   int mode = 0);
};

} // namespace mini_net

#endif // MINI_NET_LAYER_
